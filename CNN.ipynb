{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 2 classes.\n",
      "Found 1295 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\Drishya\\AppData\\Local\\Temp/ipykernel_18392/369129026.py:48: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/15\n",
      "11/79 [===>..........................] - ETA: 1:10 - loss: 0.7183 - accuracy: 0.5455"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 120s 2s/step - loss: 0.2876 - accuracy: 0.8683 - val_loss: 0.5587 - val_accuracy: 0.8008\n",
      "Epoch 2/15\n",
      "79/79 [==============================] - 101s 1s/step - loss: 0.1120 - accuracy: 0.9623 - val_loss: 0.6971 - val_accuracy: 0.7822\n",
      "Epoch 3/15\n",
      "79/79 [==============================] - 101s 1s/step - loss: 0.0741 - accuracy: 0.9734 - val_loss: 1.2987 - val_accuracy: 0.6749\n",
      "Epoch 4/15\n",
      "79/79 [==============================] - 98s 1s/step - loss: 0.0779 - accuracy: 0.9758 - val_loss: 1.5119 - val_accuracy: 0.6471\n",
      "Epoch 5/15\n",
      "79/79 [==============================] - 103s 1s/step - loss: 0.0670 - accuracy: 0.9778 - val_loss: 0.9227 - val_accuracy: 0.7359\n",
      "Epoch 6/15\n",
      "79/79 [==============================] - 106s 1s/step - loss: 0.0720 - accuracy: 0.9774 - val_loss: 0.6760 - val_accuracy: 0.7992\n",
      "Epoch 7/15\n",
      "79/79 [==============================] - 103s 1s/step - loss: 0.0758 - accuracy: 0.9770 - val_loss: 1.1931 - val_accuracy: 0.6618\n",
      "Epoch 8/15\n",
      "79/79 [==============================] - 97s 1s/step - loss: 0.0603 - accuracy: 0.9813 - val_loss: 1.3717 - val_accuracy: 0.6409\n",
      "Epoch 9/15\n",
      "79/79 [==============================] - 211s 3s/step - loss: 0.0543 - accuracy: 0.9841 - val_loss: 1.0091 - val_accuracy: 0.7097\n",
      "Epoch 10/15\n",
      "79/79 [==============================] - 164s 2s/step - loss: 0.0652 - accuracy: 0.9786 - val_loss: 0.9637 - val_accuracy: 0.7761\n",
      "Epoch 11/15\n",
      "79/79 [==============================] - 117s 1s/step - loss: 0.0395 - accuracy: 0.9873 - val_loss: 1.2833 - val_accuracy: 0.6672\n",
      "Epoch 12/15\n",
      "79/79 [==============================] - 102s 1s/step - loss: 0.0441 - accuracy: 0.9873 - val_loss: 1.6333 - val_accuracy: 0.6687\n",
      "Epoch 13/15\n",
      "79/79 [==============================] - 97s 1s/step - loss: 0.0367 - accuracy: 0.9853 - val_loss: 1.6849 - val_accuracy: 0.6857\n",
      "Epoch 14/15\n",
      "79/79 [==============================] - 98s 1s/step - loss: 0.0413 - accuracy: 0.9893 - val_loss: 1.0771 - val_accuracy: 0.7544\n",
      "Epoch 15/15\n",
      "79/79 [==============================] - 94s 1s/step - loss: 0.0384 - accuracy: 0.9869 - val_loss: 1.2253 - val_accuracy: 0.7629\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as k\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,SpatialDropout2D,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "\n",
    "# BUILDING MODEL TO CLASSIFY BETWEEN MASK AND NO MASK\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D() )\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D() )\n",
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D() )\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'D:/DATASETS/datasets/FaceMaskDataset/train/',\n",
    "        target_size=(224,224),\n",
    "        batch_size=32 ,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'D:/DATASETS/datasets/FaceMaskDataset/test/',\n",
    "        target_size=(224,224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "model_saved=model.fit_generator(\n",
    "        training_set,\n",
    "        epochs=15,\n",
    "        validation_data=test_set,\n",
    "\n",
    "        )\n",
    "\n",
    "model.save('myy_model.h5',model_saved)\n",
    "#To test for individual images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.225259780883789, 0.7629343867301941]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.resize(cv2.imread(\"D:/DATASETS/datasets/FaceMaskDataset/test/without_mask/100.jpg\", cv2.IMREAD_GRAYSCALE), (IMG_SIZE,IMG_SIZE))\n",
    "test_image = np.array(test_image).reshape( -1, IMG_SIZE, IMG_SIZE, 1)\n",
    "prediction = model.predict({'input': test_image })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPLEMENTING LIVE DETECTION OF FACE MASK\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.backend as k\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,SpatialDropout2D,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "mymodel=load_model('myy_model.h5')\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascade=cv2.CascadeClassifier('D:/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _,img=cap.read()\n",
    "    face=face_cascade.detectMultiScale(img,scaleFactor=1.1,minNeighbors=4)\n",
    "    for(x,y,w,h) in face:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite('temp.jpg',face_img)\n",
    "        test_image=image.load_img('temp.jpg',target_size=(224,224,3))\n",
    "        test_image=image.img_to_array(test_image)\n",
    "        test_image=np.expand_dims(test_image,axis=0)\n",
    "        pred=mymodel.predict(test_image)[0][0]\n",
    "        if pred==1:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "            cv2.putText(img,'NO MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "        else:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            cv2.putText(img,'MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
    "        datet=str(datetime.datetime.now())\n",
    "        cv2.putText(img,datet,(400,450),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "          \n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
