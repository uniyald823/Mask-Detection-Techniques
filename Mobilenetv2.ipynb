{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "446b7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import cv2\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Flatten, Dropout, Conv2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0774615",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = ImageDataGenerator(\n",
    "rescale=1/255.,\n",
    "horizontal_flip=True,\n",
    "validation_split = 0.1\n",
    ")\n",
    "testGen =ImageDataGenerator(\n",
    "rescale= 1/255.,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b4e95c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2268 images belonging to 2 classes.\n",
      "Found 128 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = trainGen.flow_from_directory(\n",
    "    'D:/DATASETS/datasets/FaceMaskDataset/train/', \n",
    "    target_size=(224, 224),\n",
    "    classes=['with_mask','without_mask'],\n",
    "    class_mode='categorical', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation = trainGen.flow_from_directory(\n",
    "    'D:/DATASETS/datasets/FaceMaskDataset/test/', \n",
    "    target_size=(224, 224),\n",
    "    classes=['with_mask','without_mask'],\n",
    "    class_mode='categorical', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e1d670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mob = MobileNetV2(\n",
    "    input_shape = (224,224,3),\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    ")\n",
    "mob.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d348b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_4 ( (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                81984     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,340,098\n",
      "Trainable params: 82,114\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maskmodel = Sequential()\n",
    "maskmodel.add(mob)\n",
    "maskmodel.add(GlobalAveragePooling2D())\n",
    "maskmodel.add(Dense(64,activation='relu'))\n",
    "maskmodel.add(Dropout(0.3))\n",
    "maskmodel.add(Dense(2,activation='softmax'))\n",
    "maskmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b0ed84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskmodel.compile(optimizer=Adam(),loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73afc15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "64/71 [==========================>...] - ETA: 4s - loss: 0.0253 - acc: 0.9922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning:\n",
      "\n",
      "Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - ETA: 0s - loss: 0.0249 - acc: 0.9921\n",
      "Epoch 00001: val_loss improved from inf to 0.12918, saving model to model.h5\n",
      "71/71 [==============================] - 58s 816ms/step - loss: 0.0249 - acc: 0.9921 - val_loss: 0.1292 - val_acc: 0.9531\n",
      "Epoch 2/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 00002: val_loss did not improve from 0.12918\n",
      "71/71 [==============================] - 55s 769ms/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.3218 - val_acc: 0.8906\n",
      "Epoch 3/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0121 - acc: 0.9965\n",
      "Epoch 00003: val_loss did not improve from 0.12918\n",
      "71/71 [==============================] - 52s 727ms/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.1484 - val_acc: 0.9531\n",
      "Epoch 4/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0036 - acc: 0.9987\n",
      "Epoch 00004: val_loss improved from 0.12918 to 0.12314, saving model to model.h5\n",
      "71/71 [==============================] - 51s 722ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.1231 - val_acc: 0.9609\n",
      "Epoch 5/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0030 - acc: 0.9991\n",
      "Epoch 00005: val_loss improved from 0.12314 to 0.10597, saving model to model.h5\n",
      "71/71 [==============================] - 52s 734ms/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1060 - val_acc: 0.9766\n",
      "Epoch 6/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0070 - acc: 0.9969\n",
      "Epoch 00006: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 50s 706ms/step - loss: 0.0070 - acc: 0.9969 - val_loss: 0.1764 - val_acc: 0.9453\n",
      "Epoch 7/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
      "Epoch 00007: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 52s 728ms/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.1567 - val_acc: 0.9531\n",
      "Epoch 8/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0035 - acc: 0.9991\n",
      "Epoch 00008: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 52s 734ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.1363 - val_acc: 0.9688\n",
      "Epoch 9/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 3.9980e-04 - acc: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 58s 817ms/step - loss: 3.9980e-04 - acc: 1.0000 - val_loss: 0.3258 - val_acc: 0.9141\n",
      "Epoch 10/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 4.8138e-04 - acc: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 86s 1s/step - loss: 4.8138e-04 - acc: 1.0000 - val_loss: 0.2875 - val_acc: 0.9219\n",
      "Epoch 11/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 00011: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 55s 771ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.2731 - val_acc: 0.9141\n",
      "Epoch 12/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 00012: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 53s 750ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.2185 - val_acc: 0.9531\n",
      "Epoch 13/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9982\n",
      "Epoch 00013: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 53s 742ms/step - loss: 0.0034 - acc: 0.9982 - val_loss: 0.1924 - val_acc: 0.9531\n",
      "Epoch 14/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0012 - acc: 0.9996\n",
      "Epoch 00014: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 50s 711ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.3781 - val_acc: 0.9062\n",
      "Epoch 15/15\n",
      "71/71 [==============================] - ETA: 0s - loss: 6.4272e-04 - acc: 1.0000\n",
      "Epoch 00015: val_loss did not improve from 0.10597\n",
      "71/71 [==============================] - 50s 708ms/step - loss: 6.4272e-04 - acc: 1.0000 - val_loss: 0.2792 - val_acc: 0.9141\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "hist = maskmodel.fit(\n",
    "    train,\n",
    "    epochs = 15,\n",
    "    validation_data = validation,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "645ca256",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskmodel.save('mobilenet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b30ac73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24498730897903442, 0.9296875]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskmodel.evaluate(validation,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25dbca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"mobilenetv2_1.00_224_input_1:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_5_1:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTING LIVE DETECTION OF FACE MASK\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as k\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,SpatialDropout2D,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "mymodel = load_model('mobilenet_model.h5')\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascade=cv2.CascadeClassifier('D:/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _,img=cap.read()\n",
    "    face=face_cascade.detectMultiScale(img,scaleFactor=1.1,minNeighbors=4)\n",
    "    for(x,y,w,h) in face:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite('temp.jpg',face_img)\n",
    "        test_image=image.load_img('temp.jpg',target_size=(150,150,3))\n",
    "        test_image=image.img_to_array(test_image)\n",
    "        test_image=np.expand_dims(test_image,axis=0)\n",
    "        pred=mymodel.predict(test_image)[0][0]\n",
    "        if pred==1:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "            cv2.putText(img,'NO MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "        else:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            cv2.putText(img,'MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
    "        datet=str(datetime.datetime.now())\n",
    "        cv2.putText(img,datet,(400,450),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "          \n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc29487f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
