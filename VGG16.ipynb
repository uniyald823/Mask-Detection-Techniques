{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464b7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6647ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import cv2\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Flatten, Dropout, Conv2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e7f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f8488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = 'D:/DATASETS/datasets/FaceMaskDataset/'\n",
    "MASKPATH = 'D:/DATASETS/datasets/FaceMaskDataset/with_mask/'\n",
    "NOMASKPATH = 'D:/DATASETS/datasets/FaceMaskDataset/without_mask/'\n",
    "TESTPATH = 'D:/DATASETS/datasets/FaceMaskDataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f048ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(pth):\n",
    "    images=list()\n",
    "    for img in random.sample(os.listdir(pth),9):\n",
    "        images.append(img)\n",
    "        i = 0\n",
    "        fig,ax = plt.subplots(nrows=3, ncols=3, figsize=(30,20))\n",
    "    for row in range(3):\n",
    "        for col in range(3):\n",
    "            ax[row,col].imshow(cv2.imread(os.path.join(pth,images[i])))\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37bebd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1897, 1918)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(MASKPATH)),len(os.listdir(NOMASKPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5266512",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496b13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = ImageDataGenerator(\n",
    "rescale=1/255.,\n",
    "horizontal_flip=True,\n",
    "validation_split = 0.1\n",
    ")\n",
    "testGen =ImageDataGenerator(\n",
    "rescale= 1/255.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0c10547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3435 images belonging to 2 classes.\n",
      "Found 380 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = trainGen.flow_from_directory(\n",
    "    DATAPATH, \n",
    "    target_size=(224, 224),\n",
    "    classes=['with_mask','without_mask'],\n",
    "    class_mode='categorical', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation = trainGen.flow_from_directory(\n",
    "    DATAPATH, \n",
    "    target_size=(224, 224),\n",
    "    classes=['with_mask','without_mask'],\n",
    "    class_mode='categorical', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "581e4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 224 \n",
    "IMG_HEIGHT = 224\n",
    "IMG_DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ecc8fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG16(weights='C:/Users/Drishya/Downloads/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                  include_top=False, \n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
    "                 )\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268bef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 14,747,650\n",
      "Trainable params: 14,747,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maskvgg = Sequential()\n",
    "maskvgg.add(conv_base)\n",
    "maskvgg.add(GlobalAveragePooling2D())\n",
    "maskvgg.add(Dense(64,activation='relu'))\n",
    "maskvgg.add(Dropout(0.3))\n",
    "maskvgg.add(Dense(2,activation='softmax'))\n",
    "maskvgg.summary()\n",
    "maskvgg.compile(optimizer=Adam(),loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cff43916",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'modelvgg.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e9d0a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 69/215 [========>.....................] - ETA: 37:09 - loss: 0.7753 - acc: 0.4882"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - ETA: 0s - loss: 0.7348 - acc: 0.5039 \n",
      "Epoch 00001: val_loss improved from inf to 0.69847, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 2332s 11s/step - loss: 0.7348 - acc: 0.5039 - val_loss: 0.6985 - val_acc: 0.5026\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6789 - acc: 0.5639\n",
      "Epoch 00002: val_loss improved from 0.69847 to 0.40950, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 1795s 8s/step - loss: 0.6789 - acc: 0.5639 - val_loss: 0.4095 - val_acc: 0.8316\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.7432 - acc: 0.5668\n",
      "Epoch 00003: val_loss did not improve from 0.40950\n",
      "215/215 [==============================] - 1829s 9s/step - loss: 0.7432 - acc: 0.5668 - val_loss: 0.6938 - val_acc: 0.4842\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.7079 - acc: 0.5560\n",
      "Epoch 00004: val_loss did not improve from 0.40950\n",
      "215/215 [==============================] - 1971s 9s/step - loss: 0.7079 - acc: 0.5560 - val_loss: 0.6684 - val_acc: 0.8079\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.5436 - acc: 0.7342\n",
      "Epoch 00005: val_loss improved from 0.40950 to 0.25158, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 1977s 9s/step - loss: 0.5436 - acc: 0.7342 - val_loss: 0.2516 - val_acc: 0.9289\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.3150 - acc: 0.8900\n",
      "Epoch 00006: val_loss did not improve from 0.25158\n",
      "215/215 [==============================] - 1998s 9s/step - loss: 0.3150 - acc: 0.8900 - val_loss: 0.3427 - val_acc: 0.9026\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.2632 - acc: 0.9237\n",
      "Epoch 00007: val_loss improved from 0.25158 to 0.16730, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 2139s 10s/step - loss: 0.2632 - acc: 0.9237 - val_loss: 0.1673 - val_acc: 0.9474\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.2664 - acc: 0.9106\n",
      "Epoch 00008: val_loss did not improve from 0.16730\n",
      "215/215 [==============================] - 2110s 10s/step - loss: 0.2664 - acc: 0.9106 - val_loss: 0.1693 - val_acc: 0.9500\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.2134 - acc: 0.9316\n",
      "Epoch 00009: val_loss improved from 0.16730 to 0.15444, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 2076s 10s/step - loss: 0.2134 - acc: 0.9316 - val_loss: 0.1544 - val_acc: 0.9447\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.2536 - acc: 0.9179\n",
      "Epoch 00010: val_loss improved from 0.15444 to 0.15201, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 2126s 10s/step - loss: 0.2536 - acc: 0.9179 - val_loss: 0.1520 - val_acc: 0.9553\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1876 - acc: 0.9383\n",
      "Epoch 00011: val_loss improved from 0.15201 to 0.15184, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 1952s 9s/step - loss: 0.1876 - acc: 0.9383 - val_loss: 0.1518 - val_acc: 0.9605\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1851 - acc: 0.9374\n",
      "Epoch 00012: val_loss improved from 0.15184 to 0.13122, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 1922s 9s/step - loss: 0.1851 - acc: 0.9374 - val_loss: 0.1312 - val_acc: 0.9684\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1512 - acc: 0.9534\n",
      "Epoch 00013: val_loss did not improve from 0.13122\n",
      "215/215 [==============================] - 1991s 9s/step - loss: 0.1512 - acc: 0.9534 - val_loss: 0.1391 - val_acc: 0.9658\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1598 - acc: 0.9488\n",
      "Epoch 00014: val_loss did not improve from 0.13122\n",
      "215/215 [==============================] - 1997s 9s/step - loss: 0.1598 - acc: 0.9488 - val_loss: 0.2764 - val_acc: 0.9395\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1528 - acc: 0.9441\n",
      "Epoch 00015: val_loss improved from 0.13122 to 0.12839, saving model to modelvgg.h5\n",
      "215/215 [==============================] - 1978s 9s/step - loss: 0.1528 - acc: 0.9441 - val_loss: 0.1284 - val_acc: 0.9632\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1502 - acc: 0.9496\n",
      "Epoch 00016: val_loss did not improve from 0.12839\n",
      "215/215 [==============================] - 2045s 10s/step - loss: 0.1502 - acc: 0.9496 - val_loss: 0.1380 - val_acc: 0.9579\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1561 - acc: 0.9508\n",
      "Epoch 00017: val_loss did not improve from 0.12839\n",
      "215/215 [==============================] - 1934s 9s/step - loss: 0.1561 - acc: 0.9508 - val_loss: 0.1633 - val_acc: 0.9500\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.1885 - acc: 0.9456\n",
      "Epoch 00018: val_loss did not improve from 0.12839\n",
      "215/215 [==============================] - 1935s 9s/step - loss: 0.1885 - acc: 0.9456 - val_loss: 0.3310 - val_acc: 0.8737\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6484 - acc: 0.5502\n",
      "Epoch 00019: val_loss did not improve from 0.12839\n",
      "215/215 [==============================] - 1933s 9s/step - loss: 0.6484 - acc: 0.5502 - val_loss: 0.6934 - val_acc: 0.4974\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.4908\n",
      "Epoch 00020: val_loss did not improve from 0.12839\n",
      "215/215 [==============================] - 1924s 9s/step - loss: 0.6934 - acc: 0.4908 - val_loss: 0.6932 - val_acc: 0.4974\n"
     ]
    }
   ],
   "source": [
    "hist = maskvgg.fit(\n",
    "    train,\n",
    "    epochs = 20,\n",
    "    validation_data = validation,\n",
    "    callbacks = [checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ded90d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 49s 2s/step - loss: 0.6932 - acc: 0.4974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6932325959205627, 0.4973684251308441]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskvgg.evaluate(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc94cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskvgg.save('modelvgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "def6ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"vgg16_input_1:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_1_1:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"vgg16_input_2:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_1_2:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,SpatialDropout2D,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "mymodel = load_model('modelvgg.h5')\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascade=cv2.CascadeClassifier('D:/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _,img=cap.read()\n",
    "    face=face_cascade.detectMultiScale(img,scaleFactor=1.1,minNeighbors=4)\n",
    "    for(x,y,w,h) in face:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite('temp.jpg',face_img)\n",
    "        test_image=image.load_img('temp.jpg',target_size=(150,150,3))\n",
    "        test_image=image.img_to_array(test_image)\n",
    "        test_image=np.expand_dims(test_image,axis=0)\n",
    "        pred=mymodel.predict(test_image)[0][0]\n",
    "        if pred==1:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "            cv2.putText(img,'NO MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "        else:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            cv2.putText(img,'MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
    "        datet=str(datetime.datetime.now())\n",
    "        cv2.putText(img,datet,(400,450),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "          \n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "import cv2\n",
    "\n",
    "mymodel=load_model('modelvgg.h5')\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascade=cv2.CascadeClassifier('D:/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _,img=cap.read()\n",
    "    face=face_cascade.detectMultiScale(img,scaleFactor=1.1,minNeighbors=4)\n",
    "    for(x,y,w,h) in face:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite('temp.jpg',face_img)\n",
    "        test_image=image.load_img('temp.jpg',target_size=(150,150,3))\n",
    "        test_image=image.img_to_array(test_image)\n",
    "        test_image=np.expand_dims(test_image,axis=0)\n",
    "        pred=mymodel.predict(test_image)[0][0]\n",
    "        if pred==1:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "            cv2.putText(img,'NO MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "        else:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            cv2.putText(img,'MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
    "        datet=str(datetime.datetime.now())\n",
    "        cv2.putText(img,datet,(400,450),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "          \n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabc8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120f4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88987c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4124a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2268 images belonging to 2 classes.\n",
      "Found 128 images belonging to 2 classes.\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 14,747,650\n",
      "Trainable params: 14,747,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.7676 - acc: 0.5304\n",
      "Epoch 00001: val_loss improved from inf to 2.59916, saving model to modellvgg.h5\n",
      "142/142 [==============================] - 1366s 10s/step - loss: 0.7676 - acc: 0.5304 - val_loss: 2.5992 - val_acc: 0.4922\n",
      "Epoch 2/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.4117 - acc: 0.8311\n",
      "Epoch 00002: val_loss improved from 2.59916 to 1.98012, saving model to modellvgg.h5\n",
      "142/142 [==============================] - 1336s 9s/step - loss: 0.4117 - acc: 0.8311 - val_loss: 1.9801 - val_acc: 0.4922\n",
      "Epoch 3/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.2126 - acc: 0.9299\n",
      "Epoch 00003: val_loss did not improve from 1.98012\n",
      "142/142 [==============================] - 1317s 9s/step - loss: 0.2126 - acc: 0.9299 - val_loss: 2.8028 - val_acc: 0.4922\n",
      "Epoch 4/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1995 - acc: 0.9361\n",
      "Epoch 00004: val_loss did not improve from 1.98012\n",
      "142/142 [==============================] - 1299s 9s/step - loss: 0.1995 - acc: 0.9361 - val_loss: 2.1648 - val_acc: 0.5703\n",
      "Epoch 5/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1354 - acc: 0.9621\n",
      "Epoch 00005: val_loss did not improve from 1.98012\n",
      "142/142 [==============================] - 1295s 9s/step - loss: 0.1354 - acc: 0.9621 - val_loss: 3.0509 - val_acc: 0.5547\n",
      "Epoch 6/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1082 - acc: 0.9656\n",
      "Epoch 00006: val_loss improved from 1.98012 to 0.78722, saving model to modellvgg.h5\n",
      "142/142 [==============================] - 1289s 9s/step - loss: 0.1082 - acc: 0.9656 - val_loss: 0.7872 - val_acc: 0.7812\n",
      "Epoch 7/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0960 - acc: 0.9665\n",
      "Epoch 00007: val_loss did not improve from 0.78722\n",
      "142/142 [==============================] - 1288s 9s/step - loss: 0.0960 - acc: 0.9665 - val_loss: 2.2543 - val_acc: 0.5234\n",
      "Epoch 8/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0712 - acc: 0.9788\n",
      "Epoch 00008: val_loss did not improve from 0.78722\n",
      "142/142 [==============================] - 1338s 9s/step - loss: 0.0712 - acc: 0.9788 - val_loss: 2.3384 - val_acc: 0.5469\n",
      "Epoch 9/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.2690 - acc: 0.9189\n",
      "Epoch 00009: val_loss improved from 0.78722 to 0.73785, saving model to modellvgg.h5\n",
      "142/142 [==============================] - 1385s 10s/step - loss: 0.2690 - acc: 0.9189 - val_loss: 0.7378 - val_acc: 0.7969\n",
      "Epoch 10/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1365 - acc: 0.9581\n",
      "Epoch 00010: val_loss did not improve from 0.73785\n",
      "142/142 [==============================] - 1304s 9s/step - loss: 0.1365 - acc: 0.9581 - val_loss: 0.8965 - val_acc: 0.7578\n",
      "Epoch 11/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0780 - acc: 0.9753\n",
      "Epoch 00011: val_loss did not improve from 0.73785\n",
      "142/142 [==============================] - 1301s 9s/step - loss: 0.0780 - acc: 0.9753 - val_loss: 1.1289 - val_acc: 0.6250\n",
      "Epoch 12/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.0581 - acc: 0.9802\n",
      "Epoch 00012: val_loss did not improve from 0.73785\n",
      "142/142 [==============================] - 1303s 9s/step - loss: 0.0581 - acc: 0.9802 - val_loss: 1.7600 - val_acc: 0.5781\n",
      "Epoch 13/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1089 - acc: 0.9683\n",
      "Epoch 00013: val_loss did not improve from 0.73785\n",
      "142/142 [==============================] - 1309s 9s/step - loss: 0.1089 - acc: 0.9683 - val_loss: 1.8690 - val_acc: 0.6797\n",
      "Epoch 14/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1969 - acc: 0.9405\n",
      "Epoch 00014: val_loss did not improve from 0.73785\n",
      "142/142 [==============================] - 1350s 10s/step - loss: 0.1969 - acc: 0.9405 - val_loss: 10.3631 - val_acc: 0.5078\n",
      "Epoch 15/15\n",
      "142/142 [==============================] - ETA: 0s - loss: 0.1119 - acc: 0.9630\n",
      "Epoch 00015: val_loss did not improve from 0.73785\n",
      "142/142 [==============================] - 1337s 9s/step - loss: 0.1119 - acc: 0.9630 - val_loss: 0.8881 - val_acc: 0.6719\n"
     ]
    }
   ],
   "source": [
    "trainGen = ImageDataGenerator(\n",
    "rescale=1/255.,\n",
    "horizontal_flip=True,\n",
    "validation_split = 0.1\n",
    ")\n",
    "testGen =ImageDataGenerator(\n",
    "rescale= 1/255.,\n",
    ")\n",
    "train = trainGen.flow_from_directory(\n",
    "    'D:/DATASETS/datasets/FaceMaskDataset/train/', \n",
    "    target_size=(224, 224),\n",
    "    classes=['with_mask','without_mask'],\n",
    "    class_mode='categorical', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation = trainGen.flow_from_directory(\n",
    "    'D:/DATASETS/datasets/FaceMaskDataset/test/', \n",
    "    target_size=(224, 224),\n",
    "    classes=['with_mask','without_mask'],\n",
    "    class_mode='categorical', \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "IMG_WIDTH = 224 \n",
    "IMG_HEIGHT = 224\n",
    "IMG_DEPTH = 3\n",
    "\n",
    "conv_base = VGG16(weights='C:/Users/Drishya/Downloads/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                  include_top=False, \n",
    "                  input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
    "                 )\n",
    "conv_base.summary()\n",
    "\n",
    "maskvgg = Sequential()\n",
    "maskvgg.add(conv_base)\n",
    "maskvgg.add(GlobalAveragePooling2D())\n",
    "maskvgg.add(Dense(64,activation='relu'))\n",
    "maskvgg.add(Dropout(0.3))\n",
    "maskvgg.add(Dense(2,activation='softmax'))\n",
    "maskvgg.summary()\n",
    "maskvgg.compile(optimizer=Adam(),loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'modellvgg.h5',\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "hist = maskvgg.fit(\n",
    "    train,\n",
    "    epochs = 15,\n",
    "    validation_data = validation,\n",
    "    callbacks = [checkpoint]\n",
    ")\n",
    "maskvgg.save('modell_vgg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d726ecd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8780681490898132, 0.671875]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskvgg.evaluate(validation,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01c68556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.metrics.Mean at 0x17e29666820>,\n",
       " <tensorflow.python.keras.metrics.MeanMetricWrapper at 0x17e3cb7e220>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc62eed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"vgg16_input_5:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_2_2:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 150, 150, 3).\n"
     ]
    }
   ],
   "source": [
    "#IMPLEMENTING LIVE DETECTION OF FACE MASK\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as k\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,SpatialDropout2D,Flatten,Dropout,Dense\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import cv2\n",
    "mymodel = load_model('modell_vgg.h5')\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "face_cascade=cv2.CascadeClassifier('D:/haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    _,img=cap.read()\n",
    "    face=face_cascade.detectMultiScale(img,scaleFactor=1.1,minNeighbors=4)\n",
    "    for(x,y,w,h) in face:\n",
    "        face_img = img[y:y+h, x:x+w]\n",
    "        cv2.imwrite('temp.jpg',face_img)\n",
    "        test_image=image.load_img('temp.jpg',target_size=(150,150,3))\n",
    "        test_image=image.img_to_array(test_image)\n",
    "        test_image=np.expand_dims(test_image,axis=0)\n",
    "        pred=mymodel.predict(test_image)[0][0]\n",
    "        if pred==1:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
    "            cv2.putText(img,'NO MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
    "        else:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            cv2.putText(img,'MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
    "        datet=str(datetime.datetime.now())\n",
    "        cv2.putText(img,datet,(400,450),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
    "          \n",
    "    cv2.imshow('img',img)\n",
    "    \n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e672cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
